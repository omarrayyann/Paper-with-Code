{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eR3X8KNXVkGj"
      },
      "source": [
        "# Visual Transformer (ViT)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U1m4W8BCYCG_"
      },
      "source": [
        "**Setting up required libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GcX3S4xjEoGD",
        "outputId": "7c01f4e9-8006-4565-ed2b-2b8f3d699076"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: einops in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (0.7.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install einops\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch import nn, einsum\n",
        "import torch.nn.functional as F\n",
        "from torch import optim\n",
        "\n",
        "from einops import rearrange, repeat\n",
        "from einops.layers.torch import Rearrange\n",
        "import numpy as np\n",
        "import torchvision\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Importing Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tL2k3L-mDkDR",
        "outputId": "d278979d-1f85-4ea9-97ea-dfa4f3315afb"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(42)\n",
        "\n",
        "DOWNLOAD_PATH = 'data'\n",
        "BATCH_SIZE_TRAIN = 100\n",
        "BATCH_SIZE_TEST = 1000\n",
        "\n",
        "transform_mnist = torchvision.transforms.Compose([torchvision.transforms.ToTensor(),\n",
        "                               torchvision.transforms.Normalize((0.1307,), (0.3081,))])\n",
        "\n",
        "train_set = torchvision.datasets.MNIST(DOWNLOAD_PATH, train=True, download=True,\n",
        "                                       transform=transform_mnist)\n",
        "train_loader = torch.utils.data.DataLoader(train_set, batch_size=BATCH_SIZE_TRAIN, shuffle=True)\n",
        "\n",
        "test_set = torchvision.datasets.MNIST(DOWNLOAD_PATH, train=False, download=True,\n",
        "                                      transform=transform_mnist)\n",
        "test_loader = torch.utils.data.DataLoader(test_set, batch_size=BATCH_SIZE_TEST, shuffle=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(60000, 28, 28)\n"
          ]
        }
      ],
      "source": [
        "print(np.array(train_set.data).shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U3Zi6xFSbU5M"
      },
      "source": [
        "# ViT architecture\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Forms a tuple from a vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "def pair(t):\n",
        "    return t if isinstance(t, tuple) else (t, t)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HNkMNRs4c4Ja"
      },
      "source": [
        "We will now set up the ViT model. There will be 3 parts to this model:\n",
        "\n",
        "* A ``patch embedding'' layer that takes an image and tokenizes it. There is some amount of tensor algebra involved here (since we have to slice and dice the input appropriately), and the `einops` package is helpful. We will also add learnable positional encodings as parameters.\n",
        "* A sequence of transformer blocks. This will be a smaller scale replica of the original proposed ViT, except that we will only use 4 blocks in our model (instead of 32 in the actual ViT).\n",
        "* A (dense) classification layer at the end.\n",
        "\n",
        "Further, each transformer block consists of the following components: \n",
        "\n",
        "* A *self-attention* layer with $H$ heads, \n",
        "* A one-hidden-layer (dense) network to collapse the various heads. For the hidden neurons, the original ViT used something called a [GeLU](https://arxiv.org/pdf/1606.08415.pdf) activation function, which is a smooth approximation to the ReLU. For our example, regular ReLUs seem to be working just fine. The original ViT also used Dropout but we won't need it here.\n",
        "* *layer normalization* preceeding each of the above operations.\n",
        "\n",
        "Some care needs to be taken in making sure the various dimensions of the tensors are matched."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "YAuZ2Nm_DXCB"
      },
      "outputs": [],
      "source": [
        "def pair(t):\n",
        "    return t if isinstance(t, tuple) else (t, t)\n",
        "\n",
        "# classes\n",
        "\n",
        "class PreNorm(nn.Module):\n",
        "    def __init__(self, dim, fn):\n",
        "        super().__init__()\n",
        "        self.norm = nn.LayerNorm(dim)\n",
        "        self.fn = fn\n",
        "    def forward(self, x, **kwargs):\n",
        "        return self.fn(self.norm(x), **kwargs)\n",
        "    \n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, dim, hidden_dim, dropout = 0.):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(dim, hidden_dim),\n",
        "            nn.ReLU(), #nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim, dim),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, dim, heads = 8, dim_head = 64, dropout = 0.):\n",
        "        super().__init__()\n",
        "        inner_dim = dim_head *  heads\n",
        "        project_out = not (heads == 1 and dim_head == dim)\n",
        "\n",
        "        self.heads = heads\n",
        "        self.scale = dim_head ** -0.5\n",
        "\n",
        "        self.attend = nn.Softmax(dim = -1)\n",
        "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias = False)\n",
        "\n",
        "        self.to_out = nn.Sequential(\n",
        "            nn.Linear(inner_dim, dim),\n",
        "            nn.Dropout(dropout)\n",
        "        ) if project_out else nn.Identity()\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, n, _, h = *x.shape, self.heads\n",
        "        qkv = self.to_qkv(x).chunk(3, dim = -1)\n",
        "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = h), qkv)\n",
        "\n",
        "        dots = einsum('b h i d, b h j d -> b h i j', q, k) * self.scale\n",
        "\n",
        "        attn = self.attend(dots)\n",
        "\n",
        "        out = einsum('b h i j, b h j d -> b h i d', attn, v)\n",
        "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
        "        return self.to_out(out)\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, dim, depth, heads, dim_head, mlp_dim, dropout = 0.):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList([])\n",
        "        for _ in range(depth):\n",
        "            self.layers.append(nn.ModuleList([\n",
        "                PreNorm(dim, Attention(dim, heads = heads, dim_head = dim_head, dropout = dropout)),\n",
        "                PreNorm(dim, FeedForward(dim, mlp_dim, dropout = dropout))\n",
        "            ]))\n",
        "    def forward(self, x):\n",
        "        for attn, ff in self.layers:\n",
        "            x = attn(x) + x\n",
        "            x = ff(x) + x\n",
        "        return x\n",
        "\n",
        "class ViT(nn.Module):\n",
        "    def __init__(self, *, image_size, patch_size, num_classes, dim, depth, heads, mlp_dim, pool = 'cls', channels = 3, dim_head = 64, dropout = 0., emb_dropout = 0.):\n",
        "        super().__init__()\n",
        "\n",
        "        # Patch Embedding\n",
        "\n",
        "        image_height, image_width = pair(image_size)\n",
        "        patch_height, patch_width = pair(patch_size)\n",
        "\n",
        "        assert image_height % patch_height == 0 and image_width % patch_width == 0, 'Image dimensions must be divisible by the patch size.'\n",
        "\n",
        "        num_patches = (image_height // patch_height) * (image_width // patch_width)\n",
        "        patch_dim = channels * patch_height * patch_width\n",
        "        assert pool in {'cls', 'mean'}, 'pool type must be either cls (cls token) or mean (mean pooling)'\n",
        "\n",
        "        self.to_patch_embedding = nn.Sequential(\n",
        "            Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1 = patch_height, p2 = patch_width),\n",
        "            nn.Linear(patch_dim, dim),\n",
        "        )\n",
        "\n",
        "        # Position Embedding\n",
        "\n",
        "        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, dim))\n",
        "\n",
        "        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n",
        "        self.dropout = nn.Dropout(emb_dropout)\n",
        "\n",
        "        self.transformer = Transformer(dim, depth, heads, dim_head, mlp_dim, dropout)\n",
        "\n",
        "        self.pool = pool\n",
        "        self.to_latent = nn.Identity()\n",
        "\n",
        "        self.mlp_head = nn.Sequential(\n",
        "            nn.LayerNorm(dim),\n",
        "            nn.Linear(dim, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, img):\n",
        "        x = self.to_patch_embedding(img)\n",
        "        b, n, _ = x.shape\n",
        "\n",
        "        cls_tokens = repeat(self.cls_token, '() n d -> b n d', b = b)\n",
        "        x = torch.cat((cls_tokens, x), dim=1)\n",
        "        x += self.pos_embedding[:, :(n + 1)]\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        x = self.transformer(x)\n",
        "\n",
        "        x = x.mean(dim = 1) if self.pool == 'mean' else x[:, 0]\n",
        "\n",
        "        x = self.to_latent(x)\n",
        "        return self.mlp_head(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "7vD4p1EyWHbD"
      },
      "outputs": [],
      "source": [
        "model = ViT(image_size=28, patch_size=7, num_classes=10, channels=1, dim=64, depth=4, heads=8, mlp_dim=128)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.003)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aK7gfKWm0ggw"
      },
      "source": [
        "Let's see how the model looks like."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "riukWYK5WIkB",
        "outputId": "af6420d3-6a37-4b3f-c60f-608c7c0e772d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ViT(\n",
              "  (to_patch_embedding): Sequential(\n",
              "    (0): Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1=7, p2=7)\n",
              "    (1): Linear(in_features=49, out_features=64, bias=True)\n",
              "  )\n",
              "  (dropout): Dropout(p=0.0, inplace=False)\n",
              "  (transformer): Transformer(\n",
              "    (layers): ModuleList(\n",
              "      (0-3): 4 x ModuleList(\n",
              "        (0): PreNorm(\n",
              "          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "          (fn): Attention(\n",
              "            (attend): Softmax(dim=-1)\n",
              "            (to_qkv): Linear(in_features=64, out_features=1536, bias=False)\n",
              "            (to_out): Sequential(\n",
              "              (0): Linear(in_features=512, out_features=64, bias=True)\n",
              "              (1): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (1): PreNorm(\n",
              "          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "          (fn): FeedForward(\n",
              "            (net): Sequential(\n",
              "              (0): Linear(in_features=64, out_features=128, bias=True)\n",
              "              (1): ReLU()\n",
              "              (2): Dropout(p=0.0, inplace=False)\n",
              "              (3): Linear(in_features=128, out_features=64, bias=True)\n",
              "              (4): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (to_latent): Identity()\n",
              "  (mlp_head): Sequential(\n",
              "    (0): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "    (1): Linear(in_features=64, out_features=10, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wo5XbEtJbtiG"
      },
      "source": [
        "This is it -- 4 transformer blocks, followed by a linear classification layer. Let us quickly see how many trainable parameters are present in this model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aTN9-kFMbdXn",
        "outputId": "801965bb-93a3-419b-9553-c3383c1f420a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "597002\n"
          ]
        }
      ],
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(count_parameters(model))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UgSVq3NqcKT3"
      },
      "source": [
        "About half a million. Not too bad; the bigger NLP type models have several tens of millions of parameters. But since we are training on MNIST this should be more than sufficient."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5mnUt-GXYYv3"
      },
      "source": [
        "# Training and testing\n",
        "\n",
        "All done! We can now train the ViT model. The following again is boilerplate code.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "OXxnADQNDpgA"
      },
      "outputs": [],
      "source": [
        "def train_epoch(model, optimizer, data_loader, loss_history):\n",
        "    total_samples = len(data_loader.dataset)\n",
        "    model.train()\n",
        "\n",
        "    for i, (data, target) in enumerate(data_loader):\n",
        "        optimizer.zero_grad()\n",
        "        output = F.log_softmax(model(data), dim=1)\n",
        "        loss = F.nll_loss(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if i % 100 == 0:\n",
        "            print('[' +  '{:5}'.format(i * len(data)) + '/' + '{:5}'.format(total_samples) +\n",
        "                  ' (' + '{:3.0f}'.format(100 * i / len(data_loader)) + '%)]  Loss: ' +\n",
        "                  '{:6.4f}'.format(loss.item()))\n",
        "            loss_history.append(loss.item())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Moywc015DrAg"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, data_loader, loss_history):\n",
        "    model.eval()\n",
        "    \n",
        "    total_samples = len(data_loader.dataset)\n",
        "    correct_samples = 0\n",
        "    total_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data, target in data_loader:\n",
        "            output = F.log_softmax(model(data), dim=1)\n",
        "            loss = F.nll_loss(output, target, reduction='sum')\n",
        "            _, pred = torch.max(output, dim=1)\n",
        "            \n",
        "            total_loss += loss.item()\n",
        "            correct_samples += pred.eq(target).sum()\n",
        "\n",
        "    avg_loss = total_loss / total_samples\n",
        "    loss_history.append(avg_loss)\n",
        "    print('\\nAverage test loss: ' + '{:.4f}'.format(avg_loss) +\n",
        "          '  Accuracy:' + '{:5}'.format(correct_samples) + '/' +\n",
        "          '{:5}'.format(total_samples) + ' (' +\n",
        "          '{:4.2f}'.format(100.0 * correct_samples / total_samples) + '%)\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YbCf2V--cohk"
      },
      "source": [
        "\n",
        "The following will take a bit of time (on CPU).  Each epoch should take about 2 to 3 minutes. At the end of training, we should see upwards of 95% test accuracy. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5d0Hb-TKDwbA",
        "outputId": "ff38326b-5de2-4f52-8275-4a2fda65e5f6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 1\n",
            "[    0/60000 (  0%)]  Loss: 2.5616\n",
            "[10000/60000 ( 17%)]  Loss: 0.2047\n",
            "[20000/60000 ( 33%)]  Loss: 0.2545\n",
            "[30000/60000 ( 50%)]  Loss: 0.2170\n",
            "[40000/60000 ( 67%)]  Loss: 0.1334\n",
            "[50000/60000 ( 83%)]  Loss: 0.3676\n",
            "\n",
            "Average test loss: 0.1667  Accuracy: 9478/10000 (94.78%)\n",
            "\n",
            "Epoch: 2\n",
            "[    0/60000 (  0%)]  Loss: 0.2007\n",
            "[10000/60000 ( 17%)]  Loss: 0.1356\n",
            "[20000/60000 ( 33%)]  Loss: 0.0893\n",
            "[30000/60000 ( 50%)]  Loss: 0.1558\n",
            "[40000/60000 ( 67%)]  Loss: 0.1575\n",
            "[50000/60000 ( 83%)]  Loss: 0.0756\n",
            "\n",
            "Average test loss: 0.1192  Accuracy: 9617/10000 (96.17%)\n",
            "\n",
            "Epoch: 3\n",
            "[    0/60000 (  0%)]  Loss: 0.2037\n",
            "[10000/60000 ( 17%)]  Loss: 0.0740\n",
            "[20000/60000 ( 33%)]  Loss: 0.1706\n",
            "[30000/60000 ( 50%)]  Loss: 0.0972\n",
            "[40000/60000 ( 67%)]  Loss: 0.2323\n",
            "[50000/60000 ( 83%)]  Loss: 0.1542\n",
            "\n",
            "Average test loss: 0.0976  Accuracy: 9703/10000 (97.03%)\n",
            "\n",
            "Execution time: 222.76 seconds\n"
          ]
        }
      ],
      "source": [
        "N_EPOCHS = 3\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "\n",
        "train_loss_history, test_loss_history = [], []\n",
        "for epoch in range(1, N_EPOCHS + 1):\n",
        "    print('Epoch:', epoch)\n",
        "    train_epoch(model, optimizer, train_loader, train_loss_history)\n",
        "    evaluate(model, test_loader, test_loss_history)\n",
        "\n",
        "print('Execution time:', '{:5.2f}'.format(time.time() - start_time), 'seconds')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "35jvT53FuKNM",
        "outputId": "8078e0a9-3a02-4d5b-e27c-2a0f0e84c46c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Average test loss: 0.0976  Accuracy: 9703/10000 (97.03%)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "evaluate(model, test_loader, test_loss_history)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "vit_mnist.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
